Many of us know about the Falcon 9 rocket of the SpaceX company. Falcon 9 is the world's first reusable orbital rocket. The purpose of this project is to simulate the landing of the falcon9 rocket booster. 

First of all I will give you a rough idea of ​​how a falcon9 rocket works.

<a picture going here>

Liftoff: The rocket lifts off from the launch site.
Main Engine Cutoff (MECO): The main engines shut down after the first stage has expended its fuel.
Stage Separation: The first stage separates from the second stage.
Second Stage Ignition: The second stage engine ignites to continue propelling the payload toward orbit.
Fairing Separation: The payload fairing, which protects the payload during ascent, separates and is jettisoned.
Payload Separation: The payload separates from the second stage once it reaches its intended orbit.
Boostback Burn: The first stage performs a boostback burn, pushing it back towards the launch site or the drone ship.
Booster Flips Over: The booster flips using nitrogen thrusters to orient itself for reentry.
Reentry Burn: A three-engine burn slows the booster down as it reenters the Earth's atmosphere.
Grid Fins Deploy: Grid fins deploy to help steer the booster during its descent.
Landing Burn: A single-engine burn slows the booster down for landing.
Landing Legs Deploy: The landing legs deploy, using compressed helium to stabilize the booster for landing.
Soft Touchdown: The booster lands softly on the drone ship, with a maximum vertical speed of 6 meters per second.

This is a hugr work, I simply described here. If you link to know more about falcon9 <link>

Ok here we are focus on the landing phase. This is the one of the  most important phase in falcon9 rocket. because the main purpose of the falcon9 rocket using ecept other is, its reusability. For that we have to land it safely.

Lets see what is reinforcement learning 

As you know machine learning has 3 parts

Superviced learning
unsuperviced learning
Reinforcement learning

<a picture going here>

Reinforcement learning (RL) is a machine learning (ML) technique that trains software to make decisions to achieve the most optimal results. It mimics the trial-and-error learning process that humans use to achieve their goals. it has 5 components

Agent: This is the learner or decision maker.
Environment: This is everything the agent interacts with.
Actions: These are the choices the agent can make.
Rewards: Feedback from the environment based on the agent's actions. Positive rewards encourage the agent to repeat the action, while negative rewards discourage it.
Goal: The agent aims to maximize the total reward over time.

lets get a simple example

Imagine teaching a dog to fetch a ball:

The dog (agent) tries different actions like running, sniffing, or sitting.
The surroundings (environment) include the yard, the ball, and you.
When the dog brings the ball back (desired action), you give it a treat (reward).
Over time, the dog learns that fetching the ball gets it treats, so it becomes better at fetching.

In reinforcement learning, the agent improves its strategy (policy) by trying different actions and learning from the results to get the highest rewards possible.

Lets see how the environment works

This has two main parts

1. STATE VARIABLES

The state consists of the following variables:

1. x position
2. y position
3. angle
4. first leg ground contact indicator
5. second leg ground contact indicator
6. throttle
7. engine gimbal

If VEL_STATE is set to true, the velocities are included:

1. x velocity
2. y velocity
3. angular velocity

2. CONTROL INPUTS

Discrete control inputs are

1. gimbal left
2. gimbal right
3. throttle up
4. throttle down
5. use first control thruster
6. use second control thruster
7. no action

Continuous control inputs are:

1. gimbal (left/right)
2. throttle (up/down)
3. control thruster (left/right)

Note : As a computer science student Im not aware about all of these in rocket science. So I referenced some sources to get this knowledge as well as this custom environment structure from EmbersArc link:https://github.com/EmbersArc/gym-rocketlander. So special thanks to him


How build custom environment

Here I used openAI gym to create this enviorenment. OpenAI Gym is a free Python toolkit that provides developers with an environment for developing and testing learning agents for deep learning models. But now they launch a newer version of it called OpenAI Gymnasium. So most of previous gym libraries and dependencies were removed or customized. THen I got tuff time. over a day to recreate this custom environment to get back to work after routing so many documentations and and articles

<picture goes here>

Solving the Falcon Lander
for detailed code for the falcon lander check my GitHubuser repo

git clone https://github.com/Chathura-Ranasinghe/Falcon9-Lander-Gym

then In order to install it, simply run this command

pip install -e .

Ok now have to create custom environment. To do this I followed the given directory structure in gymnasium official documentation. And this the file structure I have build.

train.py
test.py
test-env.py
falcon-lander-gym/
  README.md
  setup.py
  falcon_lander_gym/
    __init__.py
    envs/
      __init__.py
      falcon_lander.py

After successfully creating the custom environment and the package. then I had to  check whether my custom environment work fine or not. In order to do that I use this script

<test-env.py script goes here>

Its worked fine.

Then the biggest part. Training process. Unlike other reinforcement learning project I did. This has number of various variables and controllers as I mentioned above. 

Lets see what are the agent, enviorenment, actions, rewards and goals in this project

Agent:
The agent in this project is the Falcon rocket lander. It is designed to learn how to land on a designated ship platform without crashing. The agent interacts with the environment through actions and receives rewards or penalties based on its performance.

Environment:
The environment is a 2D simulation created using the Box2D physics engine. It includes elements such as the rocket lander, water, a ship platform, and terrain. The rocket lander must navigate this environment to achieve a successful landing.

Actions:
The agent can perform the following actions:
1. Gimbal Control: Adjust the angle of the main engine nozzle to control the direction of thrust.
2. Throttle Control: Adjust the throttle of the main engine to control the power of the thrust.
3. Side Thrusters: Apply side thrust to control horizontal movement.

In continuous mode, these actions are represented as continuous values within specified ranges.

**Rewards:**
The agent receives rewards or penalties based on various factors:
1. Shaping Reward: Encourages the agent to minimize distance to the target, reduce speed, and maintain a stable angle.
2. Fuel Cost Penalty: Penalizes the agent for using fuel.
3. Landing Reward: Given if the agent successfully lands with both legs touching the ground and minimal speed.
4. Penalties: Applied for crashing, going outside the environment boundaries, or breaking landing legs.

The overall reward structure is designed to incentivize safe, efficient, and controlled landings.

Goals:
The primary goal of the agent is to learn how to land on the ship platform safely. This involves:
1. Navigating the Environment: The agent must control its descent and navigate towards the platform.
2. Managing Velocity and Angle: The agent must manage its velocity and orientation to avoid crashing.
3. Efficient Fuel Use: The agent should use fuel efficiently to maximize rewards.

By interacting with the environment and receiving feedback through rewards and penalties, the agent aims to optimize its landing strategy over time through reinforcement learning.

To achive this all I had to create well structured and robust training algorithm. In here I decided to use PPO (Proximal Policy Optimization ) reinforcement algorithm from stable baseline3, which provides open-source implementations of deep reinforcement learning (RL) algorithms in Python. 

And to enchance the training experience I used pytourch to get GPU accesolaration and as as well as because stable baseline3 use pytourch as the backend. (Stable baseline previously used tensorflow as it backend and move to the pytouch due to technical issues, I also face this problem when converting tensorflow backend base enviorenment to pytouch).

For first practice I used simple training algorithm with 1 million timesteps

env = gym.make('RocketLander-v2')
env = make_vec_env(lambda: env, n_envs=1)
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=1000000)

After one million timesteps, the output was not actualy got. This is what I got and I expected it. because I didn't fine tune any parameters.

<video going here>

This is the statics what I got

<picture goes here>

| eval/                   |             |
|    mean_ep_length       | 625         |
|    mean_reward          | -3.65       |
| time/                   |             |
|    total_timesteps      | 999500      |
| train/                  |             |
|    approx_kl            | 0.022003133 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.837      |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00266    |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.00401    |
|    std                  | 0.407       |
|    value_loss           | 4.78e-05    

for better accurate model you have to satisfy these things

High ep_len_mean and ep_rew_mean 
High explained_variance 
Low loss, value_loss, and policy_gradient_loss 
Moderate entropy_loss, approx_kl, clip_fraction, and std 
And balanced learning rate. 

In my statics you can see it has negative mean_reward which impact the model performance and accurancy. 

In order to work with this I decided to increate the total total_timesteps in to 8 million. Using same training algorithm. But i used stable baseline3 'BaseCallback' to monitor training, save models, and evaluate performance periodically.

env = gym.make('RocketLander-v2')
env = make_vec_env(lambda: env, n_envs=1)
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=8000000, callback=callback)

guess what, I got the same final output

<video going here>>
<picture goes here>

| rollout/                |            |
|    ep_len_mean          | 765        |
|    ep_rew_mean          | -3.76      |
| time/                   |            |
|    fps                  | 464        |
|    iterations           | 3907       |
|    time_elapsed         | 17234      |
|    total_timesteps      | 8001536    |
| train/                  |            |
|    approx_kl            | 0.03397111 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | 3.26       |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0197     |
|    n_updates            | 39060      |
|    policy_gradient_loss | 0.00195    |
|    std                  | 0.0871     |
|    value_loss           | 6.96e-05   |

Then I decided to finetune the model hyperparameters in order to achieve the goel.

model = PPO(
    'MlpPolicy', 
    env, 
    tensorboard_log=LOG_DIR, 
    verbose=1,
    learning_rate=0.0003,  
    n_steps=2048,  
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.98,
    clip_range=0.2,
    ent_coef=0.05,
    normalize_advantage=True
)

The output was even worse. I did able to reduce the ep_rew_mean under -0.3 but the ep_len_mean got low

<video going here>>
<picture goes here>

rollout/                |              |
|    ep_len_mean          | 277          |
|    ep_rew_mean          | -2.15        |
| time/                   |              |
|    fps                  | 2708         |
|    iterations           | 611          |
|    time_elapsed         | 7392         |
|    total_timesteps      | 20021248     |
| train/                  |              |
|    approx_kl            | 0.0039182054 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.73        |
|    explained_variance   | 0.0447       |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0155      |
|    n_updates            | 6100         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 1.55         |
|    value_loss           | 0.0059       |
-------------------------------------